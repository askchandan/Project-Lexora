# ğŸ§ª Testing Your Chatbot - Complete Guide

## ğŸ“Š Testing Levels

```
Unit Tests (14 tests)
    â†“
Integration Tests (Query system)
    â†“
Manual Testing (CLI)
    â†“
Production Testing (Real queries)
```

---

## 1ï¸âƒ£ **Automated Test Suite** (Recommended First Step)

### âœ… Run All Tests
```powershell
cd "C:\Users\Chandan Malakar\Desktop\PDF AI Chat"
python tests/test_rag.py
```

### ğŸ“‹ What Gets Tested (14 Tests):

**Specific IT Act Sections:**
1. âœ… `test_hacking_punishment` - Section 66: Hacking punishment
2. âœ… `test_cheating_using_computer_resource` - Section 66D: Computer cheating
3. âœ… `test_publishing_private_images` - Section 66E: Privacy violation
4. âœ… `test_cyberterrorism_offense` - Section 66F: Cyberterrorism
5. âœ… `test_tampering_with_source_code` - Section 65: Source code tampering
6. âœ… `test_receiving_stolen_computer` - Section 66B: Stolen device
7. âœ… `test_unauthorized_password_usage` - Section 66C: Password misuse
8. âœ… `test_protected_system_access` - Section 70: Protected system

**Section Details:**
9. âœ… `test_section_65_details` - Complete Section 65 info
10. âœ… `test_section_66_details` - Complete Section 66 info

**Response Quality:**
11. âœ… `test_response_contains_sources` - Verify source attribution
12. âœ… `test_multiple_cyber_crimes` - Handle multiple topics
13. âœ… `test_fine_amounts` - Extract punishment amounts
14. âœ… `test_imprisonment_duration` - Extract jail duration

### ğŸ“Š Expected Output:
```
[PASS] Hacking punishment
[PASS] Cheating using computer resource
[PASS] Publishing private images
[PASS] Cyberterrorism offense
[PASS] Tampering with source code
[PASS] Receiving stolen computer
[PASS] Unauthorized password usage
[PASS] Protected system access
[PASS] Section 65 details
[PASS] Section 66 details
[PASS] Response contains sources
[PASS] Multiple cyber crimes
[PASS] Fine amounts
[PASS] Imprisonment duration

âœ… ALL TESTS PASSED (14/14)
```

---

## 2ï¸âƒ£ **Interactive CLI Testing** (Manual Testing)

### ğŸš€ Start a Query Session
```powershell
python scripts/query.py "What is the punishment for hacking?"
```

### ğŸ“ Example Test Queries:

**Basic Queries:**
```bash
# Test 1: Direct question
python scripts/query.py "What is Section 66?"

# Test 2: Punishment query
python scripts/query.py "What is the imprisonment for cyberterrorism?"

# Test 3: Fine query
python scripts/query.py "What are the fines for computer cheating?"

# Test 4: Definition query
python scripts/query.py "Define unauthorized access to computer system"

# Test 5: Complex query
python scripts/query.py "What are the penalties for hacking and stealing data?"
```

### âœ… What to Check in Responses:

- âœ“ **Relevance**: Does it answer your question?
- âœ“ **Accuracy**: Is the legal information correct?
- âœ“ **Completeness**: Does it mention imprisonment AND fines?
- âœ“ **Sources**: Does it show which PDF it came from?
- âœ“ **Format**: Is it well-organized and readable?

### Example Response Format:
```
Answer:
According to Section 66 of the IT Act, unauthorized access to computer 
systems is punishable with imprisonment up to 3 years and/or fine up to 
500,000 rupees...

Sources:
- cyber_crime_data.pdf (Page 2)
```

---

## 3ï¸âƒ£ **Database Verification**

### ğŸ—„ï¸ Check Database Population
```powershell
# Reset and repopulate database
python scripts/populate_database.py --reset
```

### ğŸ“Š Expected Output:
```
Loaded 2 documents from data/
Split into X chunks
Successfully added X documents to Chroma database
```

### âœ… Verification Checks:
```python
# Open Python interpreter
python

# Check document count
from src.database.chroma_manager import ChromaManager
cm = ChromaManager(chroma_path="chroma_db")
count = cm.get_document_count()
print(f"Total documents in database: {count}")
# Expected: 100+ documents

# Test similarity search
results = cm.similarity_search("hacking punishment", k=5)
print(f"Found {len(results)} similar documents")
# Expected: 5 documents with relevance scores
```

---

## 4ï¸âƒ£ **Component Testing**

### ğŸ”Œ Test Individual Components

#### A. Embedding Factory
```python
from src.models.embedding_factory import get_embedding_function
embeddings = get_embedding_function()
print(f"Embedding model: {embeddings.model}")
# Expected: text-embedding-3-small
```

#### B. LLM Factory
```python
from src.models.llm_factory import get_llm_model
llm = get_llm_model()
response = llm.invoke("What is AI?")
print(response.content)
# Expected: LLM response about AI
```

#### C. Config Loader
```python
from src.utils import load_config
config = load_config()
print(config)
# Expected: All keys present and non-empty
```

#### D. Vector Store
```python
from src.database.chroma_manager import ChromaManager
cm = ChromaManager(chroma_path="chroma_db")
results = cm.similarity_search("hacking", k=3)
print(f"Found {len(results)} results")
# Expected: 3 results with scores
```

---

## 5ï¸âƒ£ **Performance Testing**

### âš¡ Query Response Time
```python
import time
from src.core.query_engine import QueryEngine
from src.utils import load_config

config = load_config()
engine = QueryEngine(chroma_path=config['chroma_path'])

# Test response time
start = time.time()
answer, sources = engine.query("What is hacking?")
end = time.time()

print(f"Response time: {end - start:.2f} seconds")
# Expected: 1-5 seconds
```

### ğŸ”„ Batch Testing (Multiple Queries)
```python
queries = [
    "What is hacking?",
    "Define unauthorized access",
    "What are cyber crimes?",
    "Section 65 punishment",
    "What is phishing?"
]

for query in queries:
    answer, sources = engine.query(query)
    print(f"âœ“ {query}")
```

---

## 6ï¸âƒ£ **Stress Testing**

### ğŸ’¥ Test with Edge Cases
```python
# Empty query
engine.query("")

# Very long query
engine.query("What is " * 100)

# Special characters
engine.query("What is @#$%^&*()?")

# Multiple languages (if applicable)
engine.query("à¤¹à¥ˆà¤•à¤¿à¤‚à¤— à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?")  # Hindi

# Offensive content
engine.query("Tell me how to hack systems")
```

### Expected Behavior:
- Should handle gracefully
- Return meaningful error messages
- Not crash or hang

---

## 7ï¸âƒ£ **Quality Assurance Checklist**

### âœ… Pre-Deployment Testing

- [ ] All 14 tests passing
- [ ] Database populated with 100+ documents
- [ ] Query response time < 5 seconds
- [ ] No errors in logs
- [ ] Sources properly attributed
- [ ] Embeddings generating correctly
- [ ] LLM responses relevant to queries
- [ ] Config loaded from .env
- [ ] No sensitive data in console output

### âœ… Response Quality

- [ ] Answers are legally accurate
- [ ] Responses mention Section numbers
- [ ] Penalties (fines/imprisonment) included
- [ ] Sources cited correctly
- [ ] No hallucinated information
- [ ] Formatting is readable

---

## 8ï¸âƒ£ **Debugging Failed Tests**

### ğŸ” If Tests Fail:

#### Step 1: Check Prerequisites
```powershell
# Verify Python
python --version  # Should be 3.8+

# Check dependencies
pip list | findstr "langchain openai chromadb"

# Check .env file
Get-Content .env  # Verify API key exists
```

#### Step 2: Check Database
```powershell
# Repopulate database
python scripts/populate_database.py --reset

# Verify documents loaded
python -c "from src.database.chroma_manager import ChromaManager; cm = ChromaManager(); print(f'Documents: {cm.get_document_count()}')"
```

#### Step 3: Test Components Individually
```python
# Test embeddings
from src.models.embedding_factory import get_embedding_function
emb = get_embedding_function()
test_embedding = emb.embed_query("test")
print(f"Embedding size: {len(test_embedding)}")  # Should be 1536
```

#### Step 4: Check Logs
```powershell
# View recent logs
Get-ChildItem logs/ -Recurse | Sort-Object LastWriteTime -Descending | Select-Object -First 5
```

---

## 9ï¸âƒ£ **Test Results Interpretation**

### âœ… All Tests Pass
```
Scenario: Perfect âœ“
Meaning: System working as expected
Next Step: Deploy to production
```

### âš ï¸ Some Tests Fail
```
Scenario: 10/14 tests pass
Meaning: Check specific test failures
Next Step: Debug failing components
```

### âŒ All Tests Fail
```
Scenario: 0/14 tests pass
Meaning: System configuration issue
Likely Causes:
- Database not populated
- API keys incorrect
- Dependencies missing
- .env file misconfigured
```

---

## ğŸ”Ÿ **Recommended Testing Workflow**

### 1ï¸âƒ£ Initial Setup
```powershell
# Verify installation
python tests/test_rag.py
```

### 2ï¸âƒ£ After Code Changes
```powershell
# Run tests before committing
python tests/test_rag.py
```

### 3ï¸âƒ£ Daily Development
```powershell
# Quick manual test
python scripts/query.py "Your test question"
```

### 4ï¸âƒ£ Before Deployment
```powershell
# Full verification
python scripts/populate_database.py --reset
python tests/test_rag.py
python scripts/query.py "Test query"
```

---

## ğŸ“ˆ Continuous Improvement

### Add More Tests
Create `tests/test_advanced.py`:
```python
def test_performance():
    """Test response time < 3 seconds"""
    import time
    start = time.time()
    engine.query("What is hacking?")
    duration = time.time() - start
    assert duration < 3, f"Response took {duration}s"

def test_context_window():
    """Test with very long queries"""
    long_query = "What is " * 50 + "hacking?"
    response, _ = engine.query(long_query)
    assert len(response) > 0
```

### Monitor Metrics
- Response times
- Accuracy rates
- Source relevance scores
- User satisfaction

---

## ğŸ¯ Summary

| Testing Type | Command | Frequency | Purpose |
|-------------|---------|-----------|---------|
| Automated | `python tests/test_rag.py` | Every commit | Verify functionality |
| Manual | `python scripts/query.py "?"` | Daily | Quality check |
| Integration | Run all scripts | Weekly | Full system test |
| Performance | Measure response time | Weekly | Optimization |
| Regression | Run old test queries | Monthly | Prevent bugs |

---

**Happy Testing! ğŸš€**
