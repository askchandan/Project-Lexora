# Project Lexora - Complete Project Structure Overview

## ğŸ“ Directory Tree

```
project-lexora/
â”‚
â”œâ”€â”€ src/                          # ğŸ¯ Main Application Code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                     # ğŸ§  Core RAG Functionality
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ rag_pipeline.py       # Document loading, splitting, DB population
â”‚   â”‚   â””â”€â”€ query_engine.py       # Query processing and response generation
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                   # ğŸ¤– Model & Embedding Management
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ embedding_factory.py  # OpenAI embeddings creation
â”‚   â”‚   â””â”€â”€ llm_factory.py        # LLM model configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ database/                 # ğŸ’¾ Vector Store Management
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ vector_store.py       # Abstract interface
â”‚   â”‚   â””â”€â”€ chroma_manager.py     # Chroma implementation
â”‚   â”‚
â”‚   â””â”€â”€ utils/                    # ğŸ› ï¸ Utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config_loader.py      # Configuration management
â”‚       â””â”€â”€ logger.py             # Logging utility
â”‚
â”œâ”€â”€ scripts/                      # ğŸš€ Executable Scripts
â”‚   â”œâ”€â”€ populate_database.py      # Load PDFs â†’ Create embeddings â†’ Store
â”‚   â””â”€â”€ query.py                  # Execute queries against RAG system
â”‚
â”œâ”€â”€ tests/                        # âœ… Test Suite (14 Tests)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_rag.py              # Comprehensive test cases
â”‚
â”œâ”€â”€ data/                         # ğŸ“„ PDF Documents (Input)
â”‚   â”œâ”€â”€ cyber_crime_data.pdf
â”‚   â””â”€â”€ ipc_sections_data.pdf
â”‚
â”œâ”€â”€ chroma_db/                    # ğŸ—„ï¸ Vector Database (Auto-generated)
â”‚   â””â”€â”€ [Chroma persistent storage]
â”‚
â”œâ”€â”€ docs/                         # ğŸ“š Documentation
â”‚   â”œâ”€â”€ PROJECT_STRUCTURE.md      # This file
â”‚   â”œâ”€â”€ ARCHITECTURE.md           # System design details
â”‚   â”œâ”€â”€ USAGE_GUIDE.md            # Step-by-step usage
â”‚   â””â”€â”€ TEST_DOCUMENTATION.md     # Test suite details
â”‚
â”œâ”€â”€ config/                       # âš™ï¸ Configuration Files
â”‚   â””â”€â”€ [Placeholder for future config files]
â”‚
â”œâ”€â”€ .env                          # ğŸ” Environment Variables (Not in repo)
â”œâ”€â”€ .gitignore                    # Git ignore rules
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ LICENSE                       # MIT License
â”œâ”€â”€ README.md                     # Project overview
â””â”€â”€ venv/                         # Virtual environment (excluded)
```

## ğŸ¯ Module Purpose & Responsibilities

### src/core/ - Core Application Logic

**rag_pipeline.py** (RAGPipeline Class)
- Purpose: Manage document processing workflow
- Responsibilities:
  - Load PDFs from directory
  - Split documents into chunks
  - Calculate unique chunk IDs
  - Add chunks to vector database
  - Clear/reset database
- Entry Point: `scripts/populate_database.py`

**query_engine.py** (QueryEngine Class)
- Purpose: Handle user queries and response generation
- Responsibilities:
  - Accept user queries
  - Retrieve relevant documents via similarity search
  - Format prompts for LLM
  - Generate responses using language model
  - Track source documents
- Entry Point: `scripts/query.py`

### src/models/ - AI Model Integration

**embedding_factory.py**
- Purpose: Create embedding functions
- Factory Pattern: Single responsibility for embedding creation
- Returns: OpenAI embeddings instance
- Used By: Both RAGPipeline and QueryEngine

**llm_factory.py**
- Purpose: Create configured LLM instances
- Configuration: Model name, temperature, max tokens, API keys
- Factory Pattern: Centralized LLM instantiation
- Supports: OpenAI, Mistral (via OpenRouter)

### src/database/ - Vector Storage

**vector_store.py** (Abstract Base Class)
- Purpose: Define interface for vector stores
- Methods: add_documents, similarity_search, delete_all
- Design Pattern: Abstract Factory for extensibility

**chroma_manager.py** (ChromaManager Class)
- Purpose: Manage Chroma vector database
- Implements: VectorStore abstract interface
- Features:
  - Add documents with IDs
  - Semantic similarity search
  - Document count tracking
  - Database reset capability

### src/utils/ - Utilities & Configuration

**config_loader.py**
- Purpose: Centralized configuration management
- Sources: Environment variables and .env file
- Returns: Dictionary with all config values
- Settings:
  - API credentials
  - Data paths
  - Model parameters
  - Temperature & token limits

**logger.py**
- Purpose: Centralized logging
- Features: Formatted output with timestamps
- Used By: All modules for consistent logging
- Levels: DEBUG, INFO, WARNING, ERROR

## ğŸ“Š Data Flow Diagrams

### Database Population Flow
```
User: python scripts/populate_database.py --reset
         â†“
RAGPipeline.clear_database() [Optional: Remove old data]
         â†“
RAGPipeline.load_documents() â†’ PyPDF reads PDFs
         â†“
RAGPipeline.split_documents() â†’ Chunks with 800 char size
         â†“
RAGPipeline._calculate_chunk_ids() â†’ Create unique IDs
         â†“
RAGPipeline.add_chunks_to_database() â†’ ChromaManager adds to DB
         â†“
ChromaManager.add_documents() â†’ Embeddings created â†’ Stored
         â†“
Console: "Successfully added X documents"
```

### Query Processing Flow
```
User: python scripts/query.py "Your question"
         â†“
QueryEngine initialized with chroma_path
         â†“
QueryEngine.query(query_text, top_k=5)
         â†“
ChromaManager.similarity_search() â†’ Top 5 similar docs
         â†“
Context extracted from documents
         â†“
Prompt formatted with context + question
         â†“
LLM.invoke(messages) â†’ ChatOpenAI generates response
         â†“
Answer + sources returned
         â†“
Console: Display answer and source PDFs
```

## ğŸ”§ Configuration Management

### Environment Variables (.env)
```env
# API Configuration
OPENAI_API_KEY=sk-...
OPENAI_API_BASE=https://openrouter.ai/api/v1

# Data Paths
DATA_PATH=data
CHROMA_PATH=chroma_db

# Model Settings
MODEL_NAME=mistralai/mistral-7b-instruct
TEMPERATURE=0.7
MAX_TOKENS=500
```

### Config Loading Sequence
```
.env File â†’ load_dotenv() â†’ load_config() â†’ Dict â†’ Used by all modules
```

## ğŸ§ª Test Suite (14 Tests)

Location: `tests/test_rag.py`

Test Categories:
1. **Specific Offenses** (8 tests)
   - Hacking punishment
   - Computer resource cheating
   - Private image publishing
   - Cyberterrorism
   - Source code tampering
   - Stolen device
   - Password misuse
   - Protected system access

2. **Section Details** (2 tests)
   - Section 65 details
   - Section 66 details

3. **Response Quality** (4 tests)
   - Response validity
   - Multiple crimes listing
   - Fine amounts extraction
   - Imprisonment duration

## ğŸ”Œ Integration Points

### External Services
1. **OpenAI/OpenRouter**: LLM and embeddings
2. **PDF Source**: Local files in `data/` directory

### Internal Integration
```
QueryEngine â† LLMFactory
QueryEngine â† ChromaManager
ChromaManager â† EmbeddingFactory
RAGPipeline â† ChromaManager
RAGPipeline â† TextSplitter
All Modules â† ConfigLoader
All Modules â† Logger
```

## ğŸ“ˆ Scalability Path

### Current Capacity
- Documents: Thousands
- Queries: Sequential
- Response Time: 2-5 seconds

### Future Scaling
1. **Async Processing**: Queue for batch document processing
2. **Caching**: Cache frequent queries
3. **Multi-Model**: Support multiple embedding models
4. **Distributed**: Separate embedding, search, and generation services
5. **UI**: Web interface for easier access

## ğŸš€ Getting Started - Quick Reference

### Setup
```bash
# 1. Install
pip install -r requirements.txt

# 2. Configure
# Edit .env with API keys

# 3. Populate
python scripts/populate_database.py --reset

# 4. Query
python scripts/query.py "Your question"

# 5. Test
python tests/test_rag.py
```

### File Locations
- Source Code: `src/`
- Scripts: `scripts/`
- Tests: `tests/`
- Data: `data/`
- Database: `chroma_db/`
- Docs: `docs/`

## ğŸ“ Module Dependencies

```
main
â”œâ”€â”€ populate_database.py
â”‚   â””â”€â”€ src.core.rag_pipeline
â”‚       â”œâ”€â”€ src.database.chroma_manager
â”‚       â”œâ”€â”€ src.models.embedding_factory
â”‚       â””â”€â”€ src.utils
â”‚
â”œâ”€â”€ query.py
â”‚   â””â”€â”€ src.core.query_engine
â”‚       â”œâ”€â”€ src.database.chroma_manager
â”‚       â”œâ”€â”€ src.models.llm_factory
â”‚       â””â”€â”€ src.utils
â”‚
â””â”€â”€ test_rag.py
    â””â”€â”€ src.core.query_engine
        â””â”€â”€ [same as query.py]
```

## âœ¨ Key Features by Module

| Module | Feature |
|--------|---------|
| RAGPipeline | Document management and indexing |
| QueryEngine | Semantic search and response generation |
| EmbeddingFactory | Consistent embedding generation |
| LLMFactory | Configurable LLM creation |
| ChromaManager | Fast vector similarity search |
| ConfigLoader | Environment-based configuration |
| Logger | Consistent logging across modules |

---

**Project Lexora** - Professional RAG System Architecture
